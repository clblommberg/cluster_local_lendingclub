{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2ec6d-cf0b-4b42-8264-3ac940a30fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "275e7706-2235-4024-b488-b5737b961091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar funciones necesarias\n",
    "from pyspark.sql.functions import col, regexp_replace, dayofmonth, weekofyear, year, month\n",
    "from pyspark.sql.functions import col, to_date, sum\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "from pyspark.sql.functions import regexp_replace, col, when\n",
    "from pyspark.sql.functions import format_number\n",
    "from pyspark.sql.types import IntegerType,FloatType\n",
    "# Puedes obtener estadísticas específicas para una columna\n",
    "from pyspark.sql.functions import rand, mean, min, max, lit\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import log1p\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81182c-6cbb-4e54-a76e-796001adb5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50debbd-5bd2-4bbf-a0b0-22a987deedd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3ce3cb-7231-487e-9ad3-9f9d6b46d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicializar la sesión de Spark como una variable global\n",
    "global spark\n",
    "spark = SparkSession.builder.appName(\"Appetl\").getOrCreate()\n",
    "\n",
    "# Definir variables globales para las rutas de archivos\n",
    "global accepted_r, rejected_r\n",
    "accepted_r = 'hdfs://namenode:8020/datasets/raw/accepted_2007_to_2018Q4.csv.gz'\n",
    "rejected_r = 'hdfs://namenode:8020/datasets/raw/rejected_2007_to_2018Q4.csv.gz'\n",
    "\n",
    "# Función para cargar los datos\n",
    "def load_data():\n",
    "    global accepted_df, rejected_df, accepted_dfm, rejected_dfm\n",
    "    # Leer los archivos CSV comprimidos usando PySpark\n",
    "    accepted_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(accepted_r)\n",
    "    rejected_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(rejected_r)\n",
    "\n",
    "    # Haciendo una copia del DataFrame usando alias\n",
    "    accepted_dfm = accepted_df.alias(\"accepted_dfm\")\n",
    "    rejected_dfm = rejected_df.alias(\"rejected_dfm\")\n",
    "\n",
    "# Llamar a la función para cargar los datos\n",
    "load_data()\n",
    "\n",
    "# Puedes acceder a accepted_df, rejected_df, accepted_dfm, rejected_dfm desde cualquier parte del notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162fc54-a153-4909-8c52-6e4268fc0467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a68afd-22c4-4285-bbe5-c59e9e5a0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar y transformar los datos\n",
    "def clean_and_transform():\n",
    "    global rejected_dfm\n",
    "    # Convertir columnas a tipos de datos adecuados y realizar transformaciones \n",
    "    rejected_dfm = rejected_dfm.withColumn(\"Risk_Score\", col(\"Risk_Score\").cast(\"float\"))\n",
    "    rejected_dfm = rejected_dfm.withColumn(\"Debt-To-Income Ratio\", regexp_replace(col(\"Debt-To-Income Ratio\"), \"%\", \"\").cast(\"float\") / 100.0)\n",
    "    rejected_dfm = rejected_dfm.withColumn(\"Zip Code\", regexp_replace(col(\"Zip Code\"), \"[^0-9]\", \"\"))\n",
    "    rejected_dfm = rejected_dfm.withColumn(\"Policy Code\", col(\"Policy Code\").cast(\"int\"))\n",
    "    # Convertir \"Amount Requested\" a tipo fecha\n",
    "    #rejected_dfm = rejected_dfm.withColumn(\"Application Date\", to_date(col(\"Application Date\"), \"MM/dd/yyyy\"))\n",
    "    \n",
    "    # Crear nuevas columnas para día, semana y mes\n",
    "    rejected_dfm = rejected_dfm.withColumn('Day', dayofmonth(col('Application Date')))\n",
    "    rejected_dfm = rejected_dfm.withColumn('Week', weekofyear(col('Application Date')))\n",
    "    rejected_dfm = rejected_dfm.withColumn('Month', month(col('Application Date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912c5f3f-3781-4082-9ed9-b9f19d51683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625eb239-731a-4fee-883c-bc21b2854183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Amount Requested: date (nullable = true)\n",
      " |-- Application Date: date (nullable = true)\n",
      " |-- Loan Title: string (nullable = true)\n",
      " |-- Risk_Score: float (nullable = true)\n",
      " |-- Debt-To-Income Ratio: double (nullable = true)\n",
      " |-- Zip Code: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Employment Length: string (nullable = true)\n",
      " |-- Policy Code: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Week: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rejected_dfm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178d4b9f-15a4-4dcf-85f5-a24a48218988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+----------+--------------------+--------+-----+-----------------+-----------+\n",
      "|Amount Requested|Application Date|          Loan Title|Risk_Score|Debt-To-Income Ratio|Zip Code|State|Employment Length|Policy Code|\n",
      "+----------------+----------------+--------------------+----------+--------------------+--------+-----+-----------------+-----------+\n",
      "|          1000.0|      2007-05-26|Wedding Covered b...|     693.0|                 0.1|     481|   NM|          4 years|          0|\n",
      "|          1000.0|      2007-05-26|  Consolidating Debt|     703.0|                 0.1|     010|   MA|         < 1 year|          0|\n",
      "|         11000.0|      2007-05-27|Want to consolida...|     715.0|                 0.1|     212|   MD|           1 year|          0|\n",
      "|          6000.0|      2007-05-27|             waksman|     698.0|  0.3863999938964844|     017|   MA|         < 1 year|          0|\n",
      "|          1500.0|      2007-05-27|              mdrigo|     509.0| 0.09430000305175781|     209|   MD|         < 1 year|          0|\n",
      "|         15000.0|      2007-05-27|          Trinfiniti|     645.0|                 0.0|     105|   NY|          3 years|          0|\n",
      "|         10000.0|      2007-05-27|         NOTIFYi Inc|     693.0|                 0.1|     210|   MD|         < 1 year|          0|\n",
      "|          3900.0|      2007-05-27|         For Justin.|     700.0|                 0.1|     469|   IN|          2 years|          0|\n",
      "|          3000.0|      2007-05-28|              title?|     694.0|                 0.1|     808|   CO|          4 years|          0|\n",
      "|          2500.0|      2007-05-28|            timgerst|     573.0| 0.11760000228881835|     407|   KY|          4 years|          0|\n",
      "|          3900.0|      2007-05-28| need to consolidate|     710.0|                 0.1|     705|   LA|        10+ years|          0|\n",
      "|          1000.0|      2007-05-28|          sixstrings|     680.0|                 0.1|     424|   KY|           1 year|          0|\n",
      "|          3000.0|      2007-05-28|          bmoore5110|     688.0|                 0.1|     190|   PA|         < 1 year|          0|\n",
      "|          1500.0|      2007-05-28|            MHarkins|     704.0|                 0.1|     189|   PA|          3 years|          0|\n",
      "|          1000.0|      2007-05-28|              Moving|     694.0|                 0.1|     354|   AL|         < 1 year|          0|\n",
      "|          8000.0|      2007-05-28|Recent College Gr...|     708.0|                 0.1|     374|   TN|         < 1 year|          0|\n",
      "|         12000.0|      2007-05-29|    FoundersCafe.com|     685.0|                 0.1|     770|   TX|          3 years|          0|\n",
      "|          1000.0|      2007-05-29|        UChicago2004|     698.0|                 0.1|     207|   MD|          3 years|          0|\n",
      "|         15000.0|      2007-05-29|Cancer is Killing...|     680.0|                 0.1|     432|   OH|         < 1 year|          0|\n",
      "|          5000.0|      2007-05-29|2006-2007 College...|     680.0|                 0.1|     011|   MA|         < 1 year|          0|\n",
      "+----------------+----------------+--------------------+----------+--------------------+--------+-----+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rejected_dfm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3e2457-6f14-4036-b0b9-446311054b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Amount Requested: double (nullable = true)\n",
      " |-- Application Date: date (nullable = true)\n",
      " |-- Loan Title: string (nullable = true)\n",
      " |-- Risk_Score: float (nullable = true)\n",
      " |-- Debt-To-Income Ratio: double (nullable = true)\n",
      " |-- Zip Code: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Employment Length: string (nullable = true)\n",
      " |-- Policy Code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rejected_dfm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b5d81a-0695-418f-b264-6b8d6da62ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "mean_risk_score = None\n",
    "mode_amount_requested = None\n",
    "\n",
    "def handle_numeric_variables(df):\n",
    "    global mean_risk_score\n",
    "    global mode_amount_requested\n",
    "\n",
    "    # Calcular la media de Risk_Score\n",
    "    mean_risk_score = df.select(mean(\"Risk_Score\")).collect()[0][0]\n",
    "\n",
    "    # Calcular la moda de Amount Requested\n",
    "    mode_amount_requested = df.groupBy(\"Amount Requested\").count().orderBy(col(\"count\").desc()).select(\"Amount Requested\").first()[0]\n",
    "\n",
    "    # Reemplazar nulos en Risk_Score por la media\n",
    "    df = df.withColumn(\"Risk_Score\", when(col(\"Risk_Score\").isNull(), lit(mean_risk_score)).otherwise(col(\"Risk_Score\")))\n",
    "\n",
    "    # Reemplazar nulos en Amount Requested por la moda\n",
    "    df = df.withColumn(\"Amount Requested\", when(col(\"Amount Requested\").isNull(), lit(mode_amount_requested)).otherwise(col(\"Amount Requested\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0508e34f-8125-49d3-b32c-97a1c02bdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_categorical_variables(df):\n",
    "    # Reemplazar los nulos en Loan Title con \"Sin registro\"\n",
    "    df = df.withColumn(\"Loan Title\", when(col(\"Loan Title\").isNull(), \"Sin registro\").otherwise(col(\"Loan Title\")))\n",
    "\n",
    "    # Reemplazar los nulos en Zip Code con 999\n",
    "    df = df.withColumn(\"Zip Code\", when(col(\"Zip Code\").isNull(), \"999\").otherwise(col(\"Zip Code\")))\n",
    "\n",
    "    # Reemplazar los nulos en State con \"Sin registro\"\n",
    "    df = df.withColumn(\"State\", when(col(\"State\").isNull(), \"Sin registro\").otherwise(col(\"State\")))\n",
    "\n",
    "    # Reemplazar los nulos en Employment Length con \"Sin registro\"\n",
    "    df = df.withColumn(\"Employment Length\", when(col(\"Employment Length\").isNull(), \"Sin registro\").otherwise(col(\"Employment Length\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d4072ed-00c8-471d-95c7-ac4269b3e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratar variables numéricas\n",
    "rejected_dfm = handle_numeric_variables(rejected_dfm)\n",
    "\n",
    "# Tratar variables categóricas\n",
    "rejected_dfm = handle_categorical_variables(rejected_dfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899d291f-981f-42a1-a84c-e16385fa0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, train_percentage=0.8):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame de PySpark.\n",
    "    - train_percentage: Porcentaje del DataFrame para el conjunto de entrenamiento.\n",
    "\n",
    "    Retorna:\n",
    "    - (train_df, test_df): Tupla de DataFrames para entrenamiento y prueba.\n",
    "    \"\"\"\n",
    "\n",
    "    # Agrega una columna 'split' con valores aleatorios entre 0 y 1\n",
    "    df_with_split = df.withColumn('split', rand())\n",
    "\n",
    "    # Divide el DataFrame en train y test según el valor en la columna 'split'\n",
    "    train_df = df_with_split.filter(col('split') <= train_percentage)\n",
    "    test_df = df_with_split.filter(col('split') > train_percentage)\n",
    "\n",
    "    # Elimina la columna 'split' ya que ya no es necesaria\n",
    "    train_df = train_df.drop('split')\n",
    "    test_df = test_df.drop('split')\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Llama a la función con tu DataFrame y porcentaje deseado\n",
    "train_df, test_df = split_train_test(rejected_dfm, train_percentage=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708a5fd-1a57-445c-85ec-6ea33f1f5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cómo crear y almacenar un DataFrame en una tabla de Hive con formato Parquet\n",
    "df.write.mode(\"overwrite\").parquet(\"/user/hive/warehouse/lending.db/train_data_parquet\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS lending.train_data_parquet USING PARQUET LOCATION '/user/hive/warehouse/lending.db/train_data_parquet'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a71aec30-1dd4-41cb-8e32-36a5bb69f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de las tablas en Hive\n",
    "hive_train_table = \"train_data\"\n",
    "hive_test_table = \"test_data\"\n",
    "\n",
    "# Crear bases de datos si no existen\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lending\")\n",
    "\n",
    "# Cambiar a la base de datos\n",
    "spark.sql(\"USE lending\")\n",
    "\n",
    "# Guardar los DataFrames como tablas en Hive\n",
    "train_df.write.mode(\"overwrite\").saveAsTable(hive_train_table)\n",
    "test_df.write.mode(\"overwrite\").saveAsTable(hive_test_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d6b05-cdf9-4e57-8386-5fc30ff06e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
