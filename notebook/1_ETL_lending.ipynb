{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2ec6d-cf0b-4b42-8264-3ac940a30fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275e7706-2235-4024-b488-b5737b961091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar funciones necesarias\n",
    "from pyspark.sql.functions import col, regexp_replace, dayofmonth, weekofyear, year, month\n",
    "from pyspark.sql.functions import col, to_date, sum\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "from pyspark.sql.functions import regexp_replace, col, when\n",
    "from pyspark.sql.functions import format_number\n",
    "from pyspark.sql.types import IntegerType,FloatType\n",
    "# Puedes obtener estadísticas específicas para una columna\n",
    "from pyspark.sql.functions import rand, mean, min, max, lit\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import log1p\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81182c-6cbb-4e54-a76e-796001adb5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50debbd-5bd2-4bbf-a0b0-22a987deedd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3ce3cb-7231-487e-9ad3-9f9d6b46d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicializar la sesión de Spark como una variable global\n",
    "global spark\n",
    "spark = SparkSession.builder.appName(\"Appetl\").getOrCreate()\n",
    "\n",
    "# Definir variables globales para las rutas de archivos\n",
    "global accepted_r, rejected_r\n",
    "accepted_r = 'hdfs://namenode:8020/datasets/raw/accepted_2007_to_2018Q4.csv.gz'\n",
    "rejected_r = 'hdfs://namenode:8020/datasets/raw/rejected_2007_to_2018Q4.csv.gz'\n",
    "\n",
    "# Función para cargar los datos\n",
    "def load_data():\n",
    "    global accepted_df, rejected_df, accepted_dfm, rejected_dfm\n",
    "    # Leer los archivos CSV comprimidos usando PySpark\n",
    "    accepted_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(accepted_r)\n",
    "    rejected_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(rejected_r)\n",
    "\n",
    "    # Haciendo una copia del DataFrame usando alias\n",
    "    accepted_dfm = accepted_df.alias(\"accepted_dfm\")\n",
    "    rejected_dfm = rejected_df.alias(\"rejected_dfm\")\n",
    "\n",
    "# Llamar a la función para cargar los datos\n",
    "load_data()\n",
    "\n",
    "# Puedes acceder a accepted_df, rejected_df, accepted_dfm, rejected_dfm desde cualquier parte del notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162fc54-a153-4909-8c52-6e4268fc0467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a68afd-22c4-4285-bbe5-c59e9e5a0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar y transformar los datos\n",
    "def clean_and_transform():\n",
    "    global rejected_dfm\n",
    "    # Convertir columnas a tipos de datos adecuados y realizar transformaciones \n",
    "    rejected_dfm = rejected_dfm.withColumn(\"Risk_Score\", col(\"Risk_Score\").cast(\"float\"))\n",
    "    rejected_dfm = rejected_dfm.withColumn(\"Debt-To-Income Ratio\", regexp_replace(col(\"Debt-To-Income Ratio\"), \"%\", \"\").cast(\"float\") / 100.0)\n",
    "    rejected_dfm = rejected_dfm.withColumn(\"Zip Code\", regexp_replace(col(\"Zip Code\"), \"[^0-9]\", \"\"))\n",
    "    rejected_dfm = rejected_dfm.withColumn(\"Policy Code\", col(\"Policy Code\").cast(\"int\"))\n",
    "    # Convertir \"Amount Requested\" a tipo fecha\n",
    "    #rejected_dfm = rejected_dfm.withColumn(\"Application Date\", to_date(col(\"Application Date\"), \"MM/dd/yyyy\"))\n",
    "    \n",
    "    # Crear nuevas columnas para día, semana y mes\n",
    "    rejected_dfm = rejected_dfm.withColumn('Day', dayofmonth(col('Application Date')))\n",
    "    rejected_dfm = rejected_dfm.withColumn('Week', weekofyear(col('Application Date')))\n",
    "    rejected_dfm = rejected_dfm.withColumn('Month', month(col('Application Date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912c5f3f-3781-4082-9ed9-b9f19d51683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625eb239-731a-4fee-883c-bc21b2854183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rejected_dfm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178d4b9f-15a4-4dcf-85f5-a24a48218988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rejected_dfm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3e2457-6f14-4036-b0b9-446311054b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rejected_dfm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b5d81a-0695-418f-b264-6b8d6da62ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "mean_risk_score = None\n",
    "mode_amount_requested = None\n",
    "\n",
    "def handle_numeric_variables(df):\n",
    "    global mean_risk_score\n",
    "    global mode_amount_requested\n",
    "\n",
    "    # Calcular la media de Risk_Score\n",
    "    mean_risk_score = df.select(mean(\"Risk_Score\")).collect()[0][0]\n",
    "\n",
    "    # Calcular la moda de Amount Requested\n",
    "    mode_amount_requested = df.groupBy(\"Amount Requested\").count().orderBy(col(\"count\").desc()).select(\"Amount Requested\").first()[0]\n",
    "\n",
    "    # Reemplazar nulos en Risk_Score por la media\n",
    "    df = df.withColumn(\"Risk_Score\", when(col(\"Risk_Score\").isNull(), lit(mean_risk_score)).otherwise(col(\"Risk_Score\")))\n",
    "\n",
    "    # Reemplazar nulos en Amount Requested por la moda\n",
    "    df = df.withColumn(\"Amount Requested\", when(col(\"Amount Requested\").isNull(), lit(mode_amount_requested)).otherwise(col(\"Amount Requested\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0508e34f-8125-49d3-b32c-97a1c02bdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_categorical_variables(df):\n",
    "    # Reemplazar los nulos en Loan Title con \"Sin registro\"\n",
    "    df = df.withColumn(\"Loan Title\", when(col(\"Loan Title\").isNull(), \"Sin registro\").otherwise(col(\"Loan Title\")))\n",
    "\n",
    "    # Reemplazar los nulos en Zip Code con 999\n",
    "    df = df.withColumn(\"Zip Code\", when(col(\"Zip Code\").isNull(), \"999\").otherwise(col(\"Zip Code\")))\n",
    "\n",
    "    # Reemplazar los nulos en State con \"Sin registro\"\n",
    "    df = df.withColumn(\"State\", when(col(\"State\").isNull(), \"Sin registro\").otherwise(col(\"State\")))\n",
    "\n",
    "    # Reemplazar los nulos en Employment Length con \"Sin registro\"\n",
    "    df = df.withColumn(\"Employment Length\", when(col(\"Employment Length\").isNull(), \"Sin registro\").otherwise(col(\"Employment Length\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d4072ed-00c8-471d-95c7-ac4269b3e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratar variables numéricas\n",
    "rejected_dfm = handle_numeric_variables(rejected_dfm)\n",
    "\n",
    "# Tratar variables categóricas\n",
    "rejected_dfm = handle_categorical_variables(rejected_dfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "899d291f-981f-42a1-a84c-e16385fa0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, train_percentage=0.8):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame de PySpark.\n",
    "    - train_percentage: Porcentaje del DataFrame para el conjunto de entrenamiento.\n",
    "\n",
    "    Retorna:\n",
    "    - (train_df, test_df): Tupla de DataFrames para entrenamiento y prueba.\n",
    "    \"\"\"\n",
    "\n",
    "    # Agrega una columna 'split' con valores aleatorios entre 0 y 1\n",
    "    df_with_split = df.withColumn('split', rand())\n",
    "\n",
    "    # Divide el DataFrame en train y test según el valor en la columna 'split'\n",
    "    train_df = df_with_split.filter(col('split') <= train_percentage)\n",
    "    test_df = df_with_split.filter(col('split') > train_percentage)\n",
    "\n",
    "    # Elimina la columna 'split' ya que ya no es necesaria\n",
    "    train_df = train_df.drop('split')\n",
    "    test_df = test_df.drop('split')\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Llama a la función con tu DataFrame y porcentaje deseado\n",
    "train_df, test_df = split_train_test(rejected_dfm, train_percentage=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39629363-4d27-45d1-9736-b552dc7673a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|  lending|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definir la ubicación del almacén Hive en HDFS\n",
    "warehouse_location = \"/user/hive/warehouse\"\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test Hive Connection\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ejecutar una consulta simple\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "030093db-a839-476f-be3c-02758ff18a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la base de datos y la tabla\n",
    "lending_database = \"lending\"\n",
    "\n",
    "lending_train = \"train_df\"  # Reemplaza con el nombre real de tu tabla\n",
    "lending_test = \"test_df\"  # Reemplaza con el nombre real de tu tabla\n",
    "\n",
    "# Cambiar a la base de datos 'lending'\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {lending_database}\")\n",
    "spark.sql(f\"USE {lending_database}\")\n",
    "\n",
    "# Guardar el DataFrame en Hive\n",
    "train_df.write.mode(\"overwrite\").saveAsTable(lending_train)\n",
    "test_df.write.mode(\"overwrite\").saveAsTable(lending_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb5a69a7-df7b-4fb7-90e0-de3e5949144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|  lending|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar una consulta simple\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bad2c890-9eac-493f-bb33-8c2caffff5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  lending|  test_df|      false|\n",
      "|  lending| train_df|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "508ff7c0-6492-4b86-a3c7-0f89fc05cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|Amount Requested            |double                      |NULL   |\n",
      "|Application Date            |date                        |NULL   |\n",
      "|Loan Title                  |string                      |NULL   |\n",
      "|Risk_Score                  |double                      |NULL   |\n",
      "|Debt-To-Income Ratio        |double                      |NULL   |\n",
      "|Zip Code                    |string                      |NULL   |\n",
      "|State                       |string                      |NULL   |\n",
      "|Employment Length           |string                      |NULL   |\n",
      "|Policy Code                 |int                         |NULL   |\n",
      "|Day                         |int                         |NULL   |\n",
      "|Week                        |int                         |NULL   |\n",
      "|Month                       |int                         |NULL   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Catalog                     |spark_catalog               |       |\n",
      "|Database                    |lending                     |       |\n",
      "|Table                       |test_df                     |       |\n",
      "|Created Time                |Mon Jan 29 02:28:44 UTC 2024|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "|Created By                  |Spark 3.5.0                 |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reemplaza 'tu_tabla' con el nombre de la tabla que deseas explorar\n",
    "tabla_seleccionada = 'test_df'\n",
    "spark.sql(f\"DESCRIBE EXTENDED {tabla_seleccionada}\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f9a6ba2-8ca4-4236-8edd-c06294f00e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|Amount Requested            |double                      |NULL   |\n",
      "|Application Date            |date                        |NULL   |\n",
      "|Loan Title                  |string                      |NULL   |\n",
      "|Risk_Score                  |double                      |NULL   |\n",
      "|Debt-To-Income Ratio        |double                      |NULL   |\n",
      "|Zip Code                    |string                      |NULL   |\n",
      "|State                       |string                      |NULL   |\n",
      "|Employment Length           |string                      |NULL   |\n",
      "|Policy Code                 |int                         |NULL   |\n",
      "|Day                         |int                         |NULL   |\n",
      "|Week                        |int                         |NULL   |\n",
      "|Month                       |int                         |NULL   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Catalog                     |spark_catalog               |       |\n",
      "|Database                    |lending                     |       |\n",
      "|Table                       |train_df                    |       |\n",
      "|Created Time                |Mon Jan 29 02:26:23 UTC 2024|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "|Created By                  |Spark 3.5.0                 |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reemplaza 'tu_tabla' con el nombre de la tabla que deseas explorar\n",
    "tabla_seleccionada = 'lending.train_df'\n",
    "spark.sql(f\"DESCRIBE EXTENDED {tabla_seleccionada}\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
